## Implementation

We are interested in the distribution of tax evasion $e$ but it cannot be observed. What is observed is the contaminated version 
$\mathcal{V}$ [@eq-ob-ev]. Evasion $e$ and the output shock $\varepsilon$ are independent [\ref{ass-ind}] with probability density distributions $f_e$ and $f_{\varepsilon}$. Then, from @def-conv

$$
f_{\mathcal{V}}(\mathcal{V})=\int f_e(\mathcal{V}+\varepsilon)f_{\varepsilon}(\varepsilon)\text{d}\varepsilon
$$

where $f_{\mathcal{V}}$ denotes the density of $\mathcal{V}$. 

### Parametric Deconvolution

Assume a functional form for $f_{\varepsilon}(\cdot;\gamma)$ that depends on known parameters $\gamma$. Assume a known functional form for the density $f_e(\cdot;\lambda)$ that depends on unknown parameters $\lambda$. We can estimate parameters $\lambda$ by

$$
\hat \lambda = \arg \max_{\lambda}\sum_{i=1}^n \log \left(\int f_e(\mathcal{V}_i+\varepsilon;\lambda)f_{\varepsilon}(\varepsilon;\gamma)\text{d}\varepsilon\right)
$$ {#eq-mle}

Properties of MLE with unobserved scalar heterogeneity have been derived elsewhere before [@Chen2007; @Yi2021].

### Non-Parametric Deconvolution by Splines

Following @Kang2021, consider the following logspline model:

$$
f_{e|\Theta}(e)=\exp(s(e;\theta)-C(\theta)), \quad L < e < R
$$

where, $[L,R]$ define the support of $f_e$, 

$$
s(e;\theta)=\sum_{j=1}^{K-1}\theta_j B_j(e),
$$

$\{B_j(E), j=1,2,\dots, K\}$ is a sequence of cubic B-spline basis functions, and 

$$
C(\theta) = \log\left(\int^R_L \exp(s(e;\theta)) \text{d}e \right) < \infty .
$$


Then $f_{e|\Theta}(e)$ is a positive density function on $(L,R)$. The logspline model assumes that $f_e$ is a twice continuously differentiable function. The last basis function is not used for identifiability purposes, given that by the property of B-splines, $\sum_{j=1}^{K}B_j(e)=1$ for all $e \in (L,R)$ .


The log likelihood of the observed variable $\mathcal{V}$ is

$$
\begin{aligned}
    l_{\mathcal{V}}(\mathbf{\theta})=&\sum_{i=1}^{n}\log \left(\int f_{\varepsilon}(e-\mathcal{V_i})f_{e|\theta}(e)\text{d}e\right)\\
    =&\sum_{i=1}^{n}\log \left(\int f_{\varepsilon}(e-\mathcal{V_i})\exp(s(e;\theta)-C(\theta))\text{d}e\right)\\
    =&\sum_{i=1}^{n}\log \left(\int f_{\varepsilon}(e-\mathcal{V_i})\exp(s(e;\theta))\text{d}e\right)-nC(\theta)
\end{aligned}
$$

The usual maximum likelihood estimate $\hat{\theta}$ is the maximizer of $l_{\mathcal{V}}(\theta)$.

To avoid overfitting, I add a penalty term to the log-likelihood function. The penalty term is a linear combination of the squared second derivatives of the B-spline basis functions. The second derivative of the B-splines is approximated by finite differences [@Eilers1996].

The penalty term is given by

$$
\text{Penalty}(\theta) = \lambda \cdot \|\mathbf{D}\theta \|^2
$$


Intuitively, we want to penalize excessive curvature in the log-density $\log f_e(e) \approx s(e; \theta) = \sum_j \theta_j B_j(e)$, where $B_j$ are B-spline basis functions.

The second derivative of a smooth function is a measure of curvature. Since we donâ€™t have an analytical form for $s''(e)$, we can approximate it using finite differences on the coefficients $\theta$.

Given a vector of spline coefficients $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$, define the second-order differences as:

$$
\Delta^2 \theta_j = \theta_j - 2\theta_{j+1} + \theta_{j+2}
$$

These values approximate the second derivative of the spline function at positions along the domain.

We can write all second differences as a matrix operation:

Let $D \in \mathbb{R}^{(k-2) \times k}$, then:

$$
D \theta = 
\begin{bmatrix}
1 & -2 & 1 & 0 & 0 & \dots & 0 \\
0 & 1 & -2 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & -2 & 1 & \dots & 0 \\
\vdots & & & \ddots & & & \vdots \\
0 & \dots & 0 & 1 & -2 & 1
\end{bmatrix}
\begin{bmatrix}
\theta_1 \\
\theta_2 \\
\theta_3 \\
\vdots \\
\theta_k
\end{bmatrix}
$$

So $D \theta$ is a vector of second differences:

$$
\left[ \theta_1 - 2\theta_2 + \theta_3, \theta_2 - 2\theta_3 + \theta_4, \dots \right]
$$

The roughness penalty becomes:

$$
\text{Penalty} = \lambda \cdot \| D \theta \|^2 = \lambda \cdot \sum_{j=1}^{k-2} (\Delta^2 \theta_j)^2
$$

where $\lambda$ is a smoothing parameter that can be choosen based on BIC/AIC or cross-validation.

The final log-likelihood function to be maximized is:

$$
l_{\mathcal{V}}(\theta) = \sum_{i=1}^{n}\log \left(\int f_{\varepsilon}(e-\mathcal{V_i})\exp(s(e;\theta))\text{d}e\right)-nC(\theta) - \lambda \cdot \| D \theta \|^2
$$

#### Simulations

Before deconvoluting with the actual data, I run some simulations to check the performance of my implmenetation in R.

I simulated a thousand observations of $\varepsilon \sim \mathcal{N}(0,0.35)$ and $\log(e) \sim \mathcal{N}(-1.8,0.95)$, and then I generated $\mathcal{V} = e - \varepsilon$. @fig-simulated-data shows the histogram of the simulated data.

![Simulated data](../Code/Products/bs_mle_hist.png){#fig-simulated-data}

I then used the logspline model to estimate the parameters of the distribution of $e$.
I used a B-spline basis with 10 knots. The knots where placed at equally spaced percentiles of the non-negative $V$ observations. For the integral, I used Gauss Legendre quadrature. To ensure good coverage of the high probability region of the density, I used a recursive function to form a grid placing a higher number of nodes in the high probability region. I set the penalty parameter $\lambda$ to 0.1. The estimated density is shown in @fig-simulated-deconv.

![LogSpline Deconvolution](../Code/Products/bs_mle.png){#fig-simulated-deconv}



<!-- Laguerre polynomials can be used to approximate any function $L_2([0,\infty), leb)$ $L_2$ norm relative to the Lebesgue measure and domain $[0,\infty)$ [@Chen2007].

The EM algorithm [@Kang2021] starts with an initial estimate $\hat{\mathbf{\theta}}^0$ and iteratively updates the estimate as follows.

**Expectation-Step**: Given the current estimate $\hat{\mathbf{\theta}}^{(k)}$ of $\hat{\mathbf{\theta}}$, calculate

$$
 b_j \left(\hat{\mathbf{\theta}}^{(k)}\right) = \sum_{i=1}^{n}\int B_j(e)f_{e|\mathcal{V},\hat{\theta}^{(k)}}(e|\mathcal{V})\text{d}e
$$

where,

$$
f_{e|\mathcal{V},\hat{\theta}}(e|\mathcal{V}) = f_{\varepsilon}(e-\mathcal{V})\exp(s(e;\theta)-C(\theta|\mathcal{V}))
$$

$$
C(\theta|\mathcal{V})=\log\left(\int f_{\varepsilon}(e-\mathcal{V})\exp(s(e;\theta))\text{d}e\right)
$$

**Maximization-Step**: Determine the updated estimate $\hat{\mathbf{\theta}}^{(k+1)}$ by maximizing

$$
Q(\mathbf{\theta}|\mathbf{\theta}^{(k)}) = \sum_{j=1}^{k_n}\theta_j b_j \left(\hat{\mathbf{\theta}}^{(k)}\right) - nC(\mathbf{\theta})
$$

The EM algorithm stops when $l_{\mathcal{V}}(\mathbf{\theta}^{(k+1)})-l_{\mathcal{V}}(\mathbf{\theta}^{(k)})<10^{-6}$. -->
