## Results

### Testing for the Presence of Tax Evasion Through Overreporting

@eq-ob-ev suggest a way to test the presence of tax evasion through cost overreporting. Let $\mathbb{E}[\mathcal{V}_{it}]\equiv \mu_{\mathcal{V}}$. Define the the null hypothesis as the absence of cost overreporting, $H_0: \mu_{\mathcal{V}}=0$, and the alternative hypothesis as the presence of cost overreporting, $H_1: \mu_{\mathcal{V}}>0$. Consequently, we can use a one sided t-test to verify for the presence of tax evasion by overreporting.

I test for the presence of tax evasion for different classifications of intermediates. Intermediates includes raw materials, energy and services. Deductibles includes raw materials and deductible expenses. Materials includes only raw materials. The items included as deductible expenses or services are detailed in @tbl-expenses-type

```{r}
#| label: tbl-evasion-test-CD
#| tbl-cap: Tax Evasion Through Cost-Overreporting One-Side t-Test by Industry in Colombia. Under the null hypothesis, there is no tax evasion. Values of the statistic were computed from @eq-ob-ev for different intermediate inputs. Standard errors shown in parethesis. Stars indicate significance level at the 1% (\\*\\*\\*), 5% (\\*\\*), and 10% (\\*).

inds_sales_tax <- tribble(
    ~ sic_3, ~ Category,
    311, "Exempt Product",
    312, "Exempt Product",
    382, "Exempt Product",
    383, "Exempt Product",
    384, "Exempt Product",
    369, "Exempt Material",
    323, "Exempt Material",
    356, "Imported Material"
)


tax_ev_test_tbl %>%
  # left_join(
  #     top_20_inds
  # ) %>%
  left_join(
      inds_sales_tax
  ) %>%
  arrange(desc(Category),sic_3) %>%
  select(
      # Category,
      sic_3,
      # description,
      log_mats_share:log_repair_maint_share
  ) %>%
  kbl(
    col.names = c("SIC", "Materials", "Electricity", "Fuels", "R&M"),
    table.attr = 'data-quarto-disable-processing="true"'
    ) %>%
    kable_classic(full_width=F, html_font = "Cambria") %>%
    # collapse_rows(columns = 1, latex_hline = "major") 
    pack_rows(
      index=c(
        "Specialized Material"=1,
        "Exempt Product"=5,
        "Exempt Material"=2,
        "Other Industries"= 12)
    )

```


@tbl-evasion-test-CD shows that the null hypothesis of no tax evasion is rejected at the 1% significance level for twelve of the top twenty manufacturing industries. 

In particular, there is no evidence of tax evasion for most industries in which products or raw materials are exempt of sales taxes, such as 312 (Other Food Products), 382 (Non-Electrical Machinery), 384 (Transport Equipment), 323 (Leather Products), and 369 (Non-Metallic Mineral Products); there is also no evidence of materials overreporting for 356 (Plastic Products)industry, whose main raw materials are likely to be specialized and supplied by few local and international suppliers.

Among the industries with exempted products, 311 (Food Products) and 383 (Electrical Machinery) there is evidence of tax evasion but as we'll see later the average overreporting is low compared to other industries. 

As expected, the evice is stronger particularly for the other industries that do not fall in the previous categories. Namely, there is evidence of tax evasion for industries 313 (Beverages); for 321 (Textiles), 322 (Wearing Apparel), and 324 (Footwear); 331 (Wood Products except Furniture) and 332 (Wood Furniture); for industry 341 (Paper) and 342 (Publishing);  351 (Industrial Chemicals) and 352 (Other Chemicals); 381 (Metal Products Except Machinery), and 390 (Other Manufacturing Industries). 

### Expenditure Swapping Tax Evasion

Although we found evidence of evasion through raw materials overreporting, the previous table would suggest that other intermediate materials could have been underreported as indicated by the negative sign and the tight standard errors. 

A closer examination reveals that non-deductible intermediates are the expenses underreported. This could be explained by what I call expenditure swapping In Colombia, firms were subject to submitting tax declarations bimonthly. This constant reporting made misreporting difficult. If firms wanted to fly unnoticed under the authority's radar their bimonthly reports had to add up as a non-evading firm. Firms could exchange non-deductible expeneses for deductible expenses. That way, if the authority challenged their fake expenses, they could come back with valid unreported expenses and only pay the sales tax owed and leave the corporate income tax unaffected.

Firms reserving valid expenses is known in the literature. It has been documented elsewhere, in particular in tax evasion studies focusing on revenue underreporting, that when authorities challenged firms' records, firms came back increasing their sales but also their expenses resulting in a zero net gain in terms of tax revenue for authorities.


```{r}
#| label: tbl-evasion-test-CD-gnr
#| tbl-cap: Tax Evasion Through Cost-Overreporting One-Side t-Test by Industry in Colombia. Under the null hypothesis, there is no tax evasion. Values of the statistic were computed from @eq-ob-ev for different intermediate inputs. Standard errors shown in parethesis. Stars indicate significance level at the 1% (\\*\\*\\*), 5% (\\*\\*), and 10% (\\*).


tax_ev_test_tbl_gnr %>%
  # left_join(
  #     top_20_inds
  # ) %>%
  left_join(
      inds_sales_tax
  ) %>%
  arrange(desc(Category),sic_3) %>%
  select(
      # Category,
      sic_3,
      # description,
      log_share:log_services_share
  ) %>%
  kbl(
    col.names = c("SIC", "M+E+S", "Materials","Electricity", "Services"),
    table.attr = 'data-quarto-disable-processing="true"'
    ) %>%
    kable_classic(full_width=F, html_font = "Cambria") %>%
    # collapse_rows(columns = 1, latex_hline = "major") %>%
    pack_rows(
      index=c(
        "Specialized Material"=1,
        "Exempt Product"=5,
        "Exempt Material"=2,
        "Other Industries"= 12)
    )

```


### Deconvoluting Tax Evasion Using Moments

One simple way to start with the deconvulution is using moments. In particular, for every $n$-th moment $\mathbb{E}[\varepsilon_{it}^n|\Theta^{NE}]=\mathbb{E}[\varepsilon_{it}^n|t]=\mathbb{E}[\varepsilon_{it}^n]$

Therefore, any moment of the tax evasion $e_{it}$ distribution $\forall t\in T$ can be estimated in theory.

Namely, from @eq-ob-ev, we can estimate the average tax evasion by
$$
\begin{aligned}
  \mathbb{E}[e_{it}|t]&=\mathbb{E}[\mathcal V_{it}|t]+\mathbb{E}[\varepsilon_{it}]\\
  &=\mathbb{E}[\mathcal V_{it}|t]+\mu_{\varepsilon}
  % \\
  % \mathbb{V}[e_{it}|t]&=\mathbb{V}[\mathcal V_{it}|t]-\mathbb{V}[\varepsilon_{it}]\\
  % &=\mathbb{V}[\mathcal V_{it}|t]-\sigma^2_{\varepsilon}
\end{aligned}
$$

Note that we learned the distribution $f_\varepsilon(\varepsilon)$ of $\varepsilon$ from the first stage, so $\mu_{\varepsilon}$ and $\sigma_{\varepsilon}$ are known.

@tbl-tax-ev-deconv-moments displays the estimated average of the log fraction that firms increase their costs of raw materials to evade taxes by claiming a greater deductible amount of their owed sales taxes.

```{r}
#| label: tbl-tax-ev-deconv-moments
#| tbl-cap: Average tax evasion by Industry. Estimates show the average tax evasion from the output shock in @eq-ob-ev.  LCI and UCI are the bias-corrected bootsrap confidence intervals at the 95\% significance level with 200 bootstrap replicates.

load("../Code/Products/boot_tax_ev_mmt.RData")

tax_ev_boot_tbl %>%
  kbl(
    col.names = c("SIC","Mean", "LCI", "UCI"),
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")

```

<!-- @fig-deconv-mmt presents the results of the first and second moments of tax evasion for the top ten industries with the highest rates of cost overreporting for different measures of inputs. Intermediates include materials, deductible and non-deductible expenses. Deductibles include materials and deductible expenses, while materials only include raw materials. -->

```{r}
#| label: fig-deconv-mmt
#| fig-cap: First and second moments of deconvoluting tax evasion from the output shock. The second moment is included in the form of the 95% confidence interval. The absolute value of the variance was used in estimating the confidence intervals because some variances were negative.
#| eval: false

load("../Code/Products/deconv.RData")

order_sic<-deconv_mmt_long%>%
    group_by(sic_3) %>%
    summarise(highest_mean=max(mean)) %>%
    arrange(desc(highest_mean)) %>%
    pull(sic_3)
deconv_mmt_long%>%
    filter( sic_3 %in% order_sic[1:10]) %>% #Only top 10 Evasion Industries
    ggplot(aes(x=factor(sic_3, levels = order_sic), y=mean, group=share,color=share, fill=share))+
    geom_point()+
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) +
    geom_hline(yintercept = 0, linetype = "dashed", linewidth=0.5) +
    coord_flip() +
    labs(x="Industry", y="Mean Evasion", title = "Mean Evasion by Industry") +
    theme_classic()+
    theme(legend.position = c(0.95, 0.95), legend.justification = c(1, 1), legend.title = element_blank())

```

<!-- Overall, deductible expenses and materials display a higher logmean cost overreporting than when looking at intermediate inputs. This implies that altough the logshare of intermediates might be closer or even lower than non-evading firms, evading firms report a higher share of deductible expenses. Depending on the industry, firms might overreport either materials or other deductible expenses to evade taxes. -->

The top five tax evading industries, 322 (Wearing Apparel), 342 (Publishing), 313 (Beverages), 351 (Industrial Chemicals), and 331 (Wood Products), display an average tax evasion $e$ greater than 12%, which is non-trivial.

Although deconvolution by moments is the simplest method, it displays the undesirable characteristic that estimate of variances frequently result with a negative sign. In the next section, I adress this problem by using parametric MLE. 

### Deconvoluting by Parametric MLE

I use parametric MLE to obtain better estimates of the variances using equation @eq-mle. I assume that the error term $\varepsilon$ follows a normal distribution. 


<!-- 
```{r}
#| label: tbl-deconv-mle
#| tbl-cap: Estimates of the first and second moments of the evasion distribution using a parametric MLE approach assuming a Normal distribution.
#| eval: false

load("../Code/Products/deconv_mle.RData")

sapply(
  names(norm_res_list),
  \(x)norm_res_list[[x]]$ev_params
  ) |> 
  t() %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")


``` 
-->

<!-- In @tbl-deconv-mle, the means coincide with the previous estimates using moments, but now the variances are positive. The corresponding confindence intervals suggest that using the absolute value of the variances obtained using moments is not unreasonable and in fact they are good approximations. -->

<!-- For tax-evasion, I first use a log-normal distribution restricting variances to be positive and compare to the previous estimates using moments. The estimates are shown in @tbl-deconv-mle -->

For tax-evasion, theory suggests that firms only have incentives to overreport costs, not to underreport them. Therefore, as explained elsewhere, it might be expected that overreporting $e\ge0$ is non-negative. In addition, it might also be expected that most firms overreport a little and a few firms overreport greater amounts. Therefore, a lognormal or a truncated normal distribution might be more appropriate.

<!-- @tbl-deconv-mle-lognormal displays the results of a parametric MLE using a lognormal distribution for the evasion random variable. -->

By definition, if a random variable $U$ is lognormal distributed, then $log(U)\sim N(\mu, \sigma)$. Thus, we cannot directly compare the parameters of the lognormal distribution to our previous estimates. We can however, used the parameters to compute any moment of the log-nomarlly distributed variable $U$ by

$$
E[U^n]=e^{n\mu+\frac{1}{2}n^2\sigma^2}
$$

In particular, the first moment and the variance are computed as follows,

$$
\begin{aligned}  
E[U]&=e^{\mu+\frac{1}{2}\sigma^2}\\
Var[U]&=E[U^2]-E[U]^2=e^{2\mu+\sigma^2}(e^{\sigma^2}-1).
\end{aligned}
$$


In addtion, the mode and the mean by

$$
\begin{aligned}
  \text{Mode}[U]&=e^{\mu-\sigma^2} \\
  \text{Med}[U]&=e^{\mu}
\end{aligned}
$$

<!-- 

To compute the confidence intervals for the mean, one practical method to approximate them is the Cox method,

$$
CI(E[U]): e^{\left(\mu+\frac{1}{2}\sigma^2 \pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma^2}{N}+\frac{\sigma^4}{2(N-1)}}\right)}
$$ 

-->


<!-- @tbl-deconv-mle displays the mean, variance, mode, median and the approximated 95% CI intervals for the mean using the Cox method. -->

<!-- 
```{r}
#| label: tbl-deconv-mle-lognormal
#| tbl-cap: Estimates of the first and second moments of the evasion distribution using a parametric MLE approach assuming a Log-Normal distribution.
#| eval: false

sapply(
  names(lognorm_res_list),
  \(x)lognorm_res_list[[x]]$ev_params
  )[-c(1,2),] |> 
  t()  %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")

```
 
 -->

Likewise, the probability density function of random variable $U$ with a normal distribution with parameters $\tilde\mu$ and $\tilde\sigma$ and truncated from below at zero is

$$
  f(u\\;\tilde\mu,\tilde\sigma) = \frac{\varphi(\frac{u-\tilde\mu}{\tilde\sigma})}{\tilde\sigma (1-\Phi(\alpha))}
$$


where 

$$
\varphi(\xi)=\frac{1}{2\pi}\exp(-\frac{1}{2}\xi^2)
$$

is the probability density function of the standard normal distribution, $\Phi(\centerdot)$ is its cumulative distribution function, and $\alpha=-\tilde\mu/\tilde\sigma$

Then, the mean becomes

$$
E[U|U>0]=\tilde\mu+\tilde\sigma\frac{\varphi(\alpha)}{1-\Phi(\alpha)}
$$

and the variance, median, and mode become

$$
\begin{aligned}  
Var[U|U>0]&=\sigma^2[1+\alpha\varphi(\alpha)/(1-\Phi(\alpha))-(\varphi(\alpha)/[1-\Phi(\alpha)])^2]\\
\text{Median}[U|U>0]&=\tilde\mu+\Phi^{-1}\left(\frac{\Phi(\alpha)+1}{2}\right)\tilde\sigma \\ 
\text{Mode}[U|U>0]&=\tilde\mu
\end{aligned}
$$



@tbl-deconv-mle-both displays the estimates of the parameters for both the lognormal and truncated normal distributions using an MLE approach. Other moments of the densities are computed as explained previously and shown for reference.

```{r}
#| label: tbl-deconv-mle-both
#| tbl-cap: Estimates of Deconvoluting Tax Evasion from the Output Shock in @eq-ob-ev by Parametric MLE Using A Log-Normal and a Truncated-Normal Distribution. $\mu$ and $\sigma$ are the parameters of the densities. Moments displayed are estimated using the distribution paramters as explained above.

load("../Code/Products/boot_deconv_mle.RData")

mle_deconv_tbl %>%
  kable(
    digits = 4,
    col.names = c("SIC","Density","$\\mu$","$\\sigma$","Mean","SD","Mode","Median"),
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")%>%
  collapse_rows(columns = 1, valign = "top")

```

Both the lognormal and the truncated normal distributions point to similar means. The differences between the other moments of the distributions can be explained by the differences in the shapes of ther probability density functions. The standard devitaion is larger in the lognormal distribution than in the truncated normal distribution which can be explained by the asymmetry of the log normal distribution which is skewed to the right. With respect to the mode, it is the same to the mean in the case of the truncated normal distribution, while it is lower than the median for the lognormal. Finally, the median is lower than the mean in the case of the lognormal, but higher than the mean in the case of the truncated normal because of the truncation from below.

Both distributions show higher estimates of the average overreporting in logs than using only moments. Now it looks like firms evade taxes by inflating their cost of their materials by 40 percent or more. 

@tbl-deconv-mle-boot shows the bias corrected bootstrap confidence intervals for the mean of each distribution by industry using 200 replicates. 


```{r}
#| label: tbl-deconv-mle-boot
#| tbl-cap: Estimates of Deconvoluting Tax Evasion from the Output Shock in @eq-ob-ev by Parametric MLE Using A Log-Normal and a Truncated-Normal Distribution. LCI and UCI are the bias corrected bootstrap confidence intervals at the 95 percent significance level using 200 replicates.

# load("../Code/Products/boot_deconv_mle.RData")

boot_mle_deconv_tbl %>%
  mutate(
    across(
      !c(SIC,Distribution),
      ~round(.x,4)
    ),
    Mean = glue::glue("{mean} [{CI_mean_LCI}, {CI_mean_UCI}]"),
    SD = glue::glue("{sd} [{CI_sd_LCI}, {CI_sd_UCI}]")
  ) %>%
  select(SIC,Density=Distribution,Mean, SD) %>%
  kable(
    # col.names = c("SIC","Density","Mean","[","]")
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  collapse_rows(columns = 1, valign = "top")
  
```


The difference between our previous estimates using moments can be explained by the fact that the log-normal and runcated distribution are restricted to positive values while we did not applied the restriction when using moments. Were we to take the average of only the positive values the mean of the moments method would be higher and closer to the log-normal and truncated normal distributions. 

From the perspective of theory, firms would have inventives to overreport their materials. Therefore, using moments to estimate overreporting without restricting to positive values like in the case of the MLE method using a lognormal or truncated distribution might underestimate tax evasion through cost overreporting. 


::: {#fig-density-plots layout-ncol="1" }

```{r}
#| warning: false
#| error: false
#| eval: true

density_plots<-lapply(
    unique(mle_deconv_tbl$sic_3),
    function(i){

      if (i==322){
        x<-seq(0,2,by=0.01)
      } else {
        x<-seq(0,1.5,by=0.01) 
      }


        mu<-mle_deconv_tbl %>% 
            filter(sic_3==i, dist=="lognormal") %>%
            pull(mu.mu)
        sigma<-mle_deconv_tbl %>% 
            filter(sic_3==i, dist=="lognormal") %>%
            pull(sigma.sigma)
        mu2<-mle_deconv_tbl %>% 
            filter(sic_3==i, dist=="truncated normal") %>%
            pull(mu.mu)
        sigma2<-mle_deconv_tbl %>% 
            filter(sic_3==i, dist=="truncated normal") %>%
            pull(sigma.sigma)

        plot(
            x,
            dnorm(x,mu2,sigma2),
            type = "l", col="red",
                    xlab = "", ylab = "",
            main = paste(
                i,
                str_sub(
                    paste0(
                        top_20_inds[top_20_inds$sic_3==i,'description'][[1]]
                    ),
                    1,30
                ),
                "..."
            )
        )
        lines(
            x,
            dlnorm(x,mu,sigma),
            type = "l", col="blue"
        )
    }
)


par(mfrow = c(5, 1)) # Establecer la disposición de la cuadrícula

for (plot in density_plots) {
   print(plot)
}

par(mfrow = c(1, 1))

```

Truncated Normal (red) and Log-Normal (blue) distribution of the overreporting of raw materials for the top tax-evading industries.

:::

### Production Function

@tbl-CD-comparision displays the results using the identifcation strategy 

```{r}
#| label: tbl-CD-comparision
#| tbl-cap: Cobb-Douglas production function parameters estimates, correcting intermediates —raw materials— for tax evasion vs. naive estimation and OLS. $h$ is a third degree polynomial. $Z=\\{k_{it},l_{it}\\}$.
#| eval: true

load("../Code/Products/deconv_prod_fun.RData")
# load("../Code/Products/Fortran_CD_GNR.RData")


cbind(evasion_tbl,CD_fortran_tbl_R[c(1,3,2)],ols_CD) %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  add_header_above(
      c(" "=1,"Tax Evasion + GNR"=3, "GNR"=3,"OLS"=3)#
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")

```




### By Year

```{r}
#| label: fig-deconv-mmt-year
#| fig-cap: First and second moments of deconvoluting tax evasion from the output shock by industry and year. The second moment is included in the form of the 95% confidence interval. The absolute value of the variance was used in estimating the confidence intervals because some variances were negative.
#| eval: false


load("../Code/Products/deconv_year.RData")

plots_year<-lapply(top_evading_inds[1:6],\(x)
    deconv_mmt_long_y%>%
        filter(sic_3==x) %>%
        ggplot(aes(x=factor(year), y=mean, group=share,color=share, fill=share))+
        geom_line()+
        geom_errorbar(aes(ymin=LCI, ymax=UCI)) +
        geom_vline(xintercept = "83", linetype = "dashed", linewidth=0.5) +
        # coord_flip() +
        labs(x="Year", y="Mean Evasion", title = "Mean Evasion by Year", subtitle = evasion_inds_description[[paste0(x)]]) +
        theme_classic()+
        theme(legend.position = "top", legend.justification = c(1, 1), legend.title = element_blank())
)

par(mfrow = c(6, 1)) # Establecer la disposición de la cuadrícula

for (plot in plots_year) {
  print(plot)
}

# Restablecer la disposición de la cuadrícula a la configuración predeterminada
par(mfrow = c(1, 1))

```

