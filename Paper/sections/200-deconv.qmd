## Results

### Testing for the Presence of Tax Evasion Through Overreporting

@eq-ob-ev suggest a way to test the presence of tax evasion through cost overreporting. Let $\mathbb{E}[\mathcal{V}_{it}]\equiv \mu_{\mathcal{V}}$. Define the the null hypothesis as the absence of cost overreporting, $H_0: \mu_{\mathcal{V}}=0$, and the alternative hypothesis as the presence of cost overreporting, $H_1: \mu_{\mathcal{V}}>0$. Consequently, we can use a one sided t-test to verify for the presence of tax evasion by overreporting.

I test for the presence of tax evasion for different classifications of intermediates. Intermediates includes raw materials, energy and services. Deductibles includes raw materials and deductible expenses. Materials includes only raw materials. The items included as deductible expenses or services are detailed in @tbl-expenses-type

```{r}
#| label: tbl-evasion-test-CD
#| tbl-cap: Tax Evasion Through Cost-Overreporting One-Side t-Test by Industry in Colombia. Under the null hypothesis, there is no tax evasion. Values of the statistic were computed from @eq-ob-ev for different classifications of flexible inputs. Standard errors shown in parethesis. Stars indicate significance level at the 1% (\\*\\*\\*), 5% (\\*\\*), and 10% (\\*).

load("../Code/Products/deconv.RData")
test_tbl_CD[,-5] %>%
  arrange(Industry) %>%
  kbl(
    table.attr = 'data-quarto-disable-processing="true"'
    ) %>%
    kable_classic(full_width=F, html_font = "Cambria")

```


@tbl-evasion-test-CD shows that the null hypothesis of no tax evasion is rejected at the 1% significance level for twelve of the top twenty manufacturing industries. The evice is stronger particularly for the materials and deductible expenses.

There is no evidence of tax evasion for industries 323 (Leather Products), 332 (Non-Metal Furniture), 384 (Transport Equipment), 369 (Non-Metallic Mineral Products), 383 (Electrical Machinery), 312 (Food Manufacturing, other than 311), and 382 (Non-Electrical Machinery), and 356 (Plastic Products).

Why? 

Food products (311 & 312) are exempt industries, low incentives to evade. 312 no evidence; 311 small value. How does the VAT credit worked with exempt industries? Most like refund from government because they did not charged VAT.

Plastic products (356) mostly using raw materials (pellets) for manufacturing, few international sellers of raw materials (mainly importing raw material?)

### Deconvoluting Tax Evasion Using Moments

One simple way to start with the deconvulution is using moments. In particular, for every $n$-th moment $\mathbb{E}[\varepsilon_{it}^n|\Theta^{NE}]=\mathbb{E}[\varepsilon_{it}^n|t]=\mathbb{E}[\varepsilon_{it}^n]$

Therefore, any moment of the tax evasion $e_{it}$ distribution $\forall t\in T$ can be estimated in theory.

Namely, from @eq-ob-ev,
$$
\begin{aligned}
  \mathbb{E}[e_{it}|t]&=\mathbb{E}[\mathcal V_{it}|t]+\mathbb{E}[\varepsilon_{it}]\\
  &=\mathbb{E}[\mathcal V_{it}|t]+\mu_{\varepsilon}
  \\
  \mathbb{V}[e_{it}|t]&=\mathbb{V}[\mathcal V_{it}|t]-\mathbb{V}[\varepsilon_{it}]\\
  &=\mathbb{V}[\mathcal V_{it}|t]-\sigma^2_{\varepsilon}
\end{aligned}
$$

We learned the distribution $f_\varepsilon(\varepsilon)$ of $\varepsilon$ from the first stage.

@fig-deconv-mmt presents the results of the first and second moments of tax evasion for the top ten industries with the highest rates of cost overreporting for different measures of inputs. Intermediates include materials, deductible and non-deductible expenses. Deductibles include materials and deductible expenses, while materials only include raw materials.

```{r}
#| label: fig-deconv-mmt
#| fig-cap: First and second moments of deconvoluting tax evasion from the output shock. The second moment is included in the form of the 95% confidence interval. The absolute value of the variance was used in estimating the confidence intervals because some variances were negative.

# load("../Code/Products/deconv.RData")

order_sic<-deconv_mmt_long%>%
    group_by(sic_3) %>%
    summarise(highest_mean=max(mean)) %>%
    arrange(desc(highest_mean)) %>%
    pull(sic_3)
deconv_mmt_long%>%
    filter( sic_3 %in% order_sic[1:10]) %>% #Only top 10 Evasion Industries
    ggplot(aes(x=factor(sic_3, levels = order_sic), y=mean, group=share,color=share, fill=share))+
    geom_point()+
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) +
    geom_hline(yintercept = 0, linetype = "dashed", linewidth=0.5) +
    coord_flip() +
    labs(x="Industry", y="Mean Evasion", title = "Mean Evasion by Industry") +
    theme_classic()+
    theme(legend.position = c(0.95, 0.95), legend.justification = c(1, 1), legend.title = element_blank())

```

Overall, deductible expenses and materials display a higher logmean cost overreporting than when looking at intermediate inputs. This implies that altough the logshare of intermediates might be closer or even lower than non-evading firms, evading firms report a higher share of deductible expenses. Depending on the industry, firms might overreport either materials or other deductible expenses to evade taxes.

Among the top ten tax evading industries, 331 (Wood Products), 351 (Industrial Chemicals), 321 (Textiles), 313 (Beverages), 342 (Publishing), 324 (Rubber/Plastic Footwear), and 322 (Wearing Apparel) display a log-average evasion $e$ equal or greater than 12%, which is non-trivial.

Some variances result negative when using moments. The absolute value of the variance was used to estimate the confidence intervals.

### Deconvoluting by Parametric MLE

I use parametric MLE to obtain better estimates of the variances using equation @eq-mle. I assume that the error term $\varepsilon$ follows a normal distribution. For tax-evasion, I first use a normal distribution restricting variances to be positive and compare to the previous estimates using moments. The estimates are shown in @tbl-deconv-mle

```{r}
#| label: tbl-deconv-mle
#| tbl-cap: Estimates of the first and second moments of the evasion distribution using a parametric MLE approach assuming a Normal distribution.

load("../Code/Products/deconv_mle.RData")

sapply(
  names(norm_res_list),
  \(x)norm_res_list[[x]]$ev_params
  ) |> 
  t() %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")


```

In @tbl-deconv-mle, the means coincide with the previous estimates using moments, but now the variances are positive. The corresponding confindence intervals suggest that using the absolute value of the variances obtained using moments is not unreasonable and in fact they are good approximations.

Although these estimates seem reasonable, theory suggess that firms only have incentives to overreport costs, not to underreport them. Therefore, as explaines elsewhere, it might be expected that overreporting $e\ge0$ is non-negative. In addition, it might also be expectd that most firms overreport a similar amount and a few might overreport high proportions. Therefore, a lognormal distribution  might be more appropriate. 

@tbl-deconv-mle-lognormal displays the results of a parametric MLE using a lognormal distribution for the evasion random variable.

By definition, if a random variable $U$ is lognormal distributed, then $log(U)\sim N(\mu, \sigma)$. Thus, we cannot directly compare the parameters of the lognormal distribution to our previous estimates. We can however, used the parameters to compute any moment of the log-nomarlly distributed variable $U$ by

$$
E[U^n]=e^{n\mu+\frac{1}{2}n^2\sigma^2}
$$

In particular, the first moment $E[U]=e^{\mu+\frac{1}{2}\sigma^2}$, the variance $Var[U]=E[U^2]-E[U]^2=e^{2\mu+\sigma^2}(e^{\sigma^2}-1)$.

In addtion, we can compute the mode and the mean by

$$
\begin{aligned}
  \text{Mode}[U]&=e^{\mu-\sigma^2} \\
  \text{Med}[U]&=e^{\mu}
\end{aligned}
$$

To compute the confidence intervals for the mean, one practical method to approximate them is the Cox method,

$$
CI(E[U]): e^{\left(\mu+\frac{1}{2}\sigma^2 \pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma^2}{N}+\frac{\sigma^4}{2(N-1)}}\right)}
$$

@tbl-deconv-mle-lognormal displays the mean, variance, mode, median and the approximated 95% CI intervals for the mean using the Cox method.


```{r}
#| label: tbl-deconv-mle-lognormal
#| tbl-cap: Estimates of the first and second moments of the evasion distribution using a parametric MLE approach assuming a Log-Normal distribution.

sapply(
  names(lognorm_res_list),
  \(x)lognorm_res_list[[x]]$ev_params
  )[-c(1,2),] |> 
  t()  %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")

```


Do these results make sense? The mean of the log-normal distribution is higher. However, when looking at the mode, the value is closer to the mean of the normal distribution. This can be explained by the fact that the log-normal distribution is restricted to positive values while the normal distribution is not. Were we to take the average of only the positive values the mean of the normal distribution would be higher and closer to the log-normal distribution. In addition, when looking at the plots of both distributions (@fig-density-plots), it is clear that both are pointing to a mass around the mode of the log-normal and the mean of the normal distribution. This is shown in @fig-density-plots for the intermediate inputs of the 322 Wearing Apparel industry.

::: {#fig-density-plots layout-ncol="1" }

```{r}
#| label: fig-density-plot-ind
#| fig-cap: Normal and Log-Normal distribution of the overreporting of deductible intermediate inputs
#| warning: false
#| error: false

invisible(
  density_plots<-lapply(
  # paste0(order_sic[1:3]," log_ded_share"),
  c("322 log_share", "324 log_ded_share", "342 log_mats_share"),
  function(x){
    if(x!="342 log_ded_share"){
      x_ax <- seq(0,2,by=0.01)
      }
    else{
      x_ax <- seq(0,1,by=0.01)
      }
    plot(
        x_ax,
        dlnorm(
            x_ax,
            lognorm_res_list[[x]]$ev_params[1],
            lognorm_res_list[[x]]$ev_params[2]
        ),
        type="l",
        col="blue",
        xlab = "", ylab = "",
        main = x
    )
    lines(
        x_ax,
        dnorm(
            x_ax,
            norm_res_list[[x]]$ev_params[1],
            norm_res_list[[x]]$ev_params[2]
        ),
        col = "red",
        xlab = "", ylab = ""
        
    )
  }
)
)

par(mfrow = c(6, 1)) # Establecer la disposición de la cuadrícula

for (plot in density_plots) {
   print(plot)
}

par(mfrow = c(1, 1))

```

Normal and Log-Normal distribution of the overreporting of deductible intermediate inputs for the top tax-evading industries

:::

### Production Function

```{r}
#| label: tbl-CD-comparision
#| tbl-cap: Cobb-Douglas production function parameters estimates, correcting intermediates —as defined in GNR— for tax evasion vs. naive estimation. One flexible input, Intermediates.

load("../Code/Products/deconv_prod_fun.RData")
load("../Code/Products/Fortran_CD_GNR.RData")

evasion_tbl<-sapply(paste0(order_sic[1:6]," log_share"),\(x)prod_fun_list[[x]]$coeffs) |> t() |> round(4)


cbind(evasion_tbl,CD_fortran_tbl_R[CD_fortran_tbl_R$inds %in% order_sic[1:6],c(1,3,2)]) %>%
  kbl(
    digits = 4,
    table.attr = 'data-quarto-disable-processing="true"'
  ) %>%
  add_header_above(
      c(" "=1,"Tax Evasion + GNR"=3, "GNR"=3)#
  ) %>%
  kable_classic(full_width=F, html_font = "Cambria")

```


### By Year

```{r}
#| label: fig-deconv-mmt-year
#| fig-cap: First and second moments of deconvoluting tax evasion from the output shock by industry and year. The second moment is included in the form of the 95% confidence interval. The absolute value of the variance was used in estimating the confidence intervals because some variances were negative.


load("../Code/Products/deconv_year.RData")

plots_year<-lapply(order_sic[1:6],\(x)
    deconv_mmt_long_y%>%
        filter(sic_3==x) %>%
        ggplot(aes(x=factor(year), y=mean, group=share,color=share, fill=share))+
        geom_line()+
        geom_errorbar(aes(ymin=LCI, ymax=UCI)) +
        geom_vline(xintercept = "83", linetype = "dashed", linewidth=0.5) +
        # coord_flip() +
        labs(x="Year", y="Mean Evasion", title = "Mean Evasion by Year", subtitle = evasion_inds_description[[paste0(x)]]) +
        theme_classic()+
        theme(legend.position = "top", legend.justification = c(1, 1), legend.title = element_blank())
)

par(mfrow = c(6, 1)) # Establecer la disposición de la cuadrícula

for (plot in plots_year) {
  print(plot)
}

# Restablecer la disposición de la cuadrícula a la configuración predeterminada
par(mfrow = c(1, 1))

```

