@workingpaper{Bustos2022,
   abstract = {Profit shifting by multinational corporations is thought to reduce tax revenue around the world. We analyze the introduction of standard regulations aimed at limiting profit shifting. Using administrative tax and customs data from Chile in difference-indifferences event-study designs, we find that the reform was ineffective in reducing multinationals' transfers to lower-tax countries and did not significantly raise tax payments. At the same time, interviews with tax advisors reveal a drastic increase in tax advisory services. The qualitative interviews also allow us to identify and then quantitatively confirm a common tax planning strategy in response to the reform. These results illustrate that when enforcement can be circumvented by sophisticated tax planning, it can benefit tax consultants at the expense of tax authorities and taxpayers.},
   author = {Sebastián Bustos and Dina Pomeranz and Juan Carlos and Suárez Serrato and José Vila-Belda and Gabriel Zucman},
   institution = {NBER},
   month = {6},
   title = {THE RACE BETWEEN TAX ENFORCEMENT AND TAX PLANNING: EVIDENCE FROM A NATURAL EXPERIMENT IN CHILE The Race Between Tax Enforcement and Tax Planning: Evidence From a Natural Experiment in Chile},
   url = {http://www.nber.org/papers/w30114},
   year = {2022},
}
@article{Ackerberg2019,
   abstract = {Much of the recent empirical work estimating production functions has used methodolo-gies proposed in two distinct lines of literature: 1) the literature started by Olley and Pakes (1996) on "proxy variable" techniques, and 2) what is commonly referred to as the "dynamic panel" literature. We illustrate how timing and …rm information set assumptions are key to both methodologies, and how these assumptions can be strengthened or weakened almost continously. We also discuss other assumptions that have utilized in these literatures to increase the precision of estimates. Empirically, we then examine how, in a number of plant level production datasets, strengthening or weakening the timing/information set assumptions a¤ects the precision of estimates. We compare these impacts on precision to those achieved by imposing other potential assumptions. This gives the researcher a better idea of the e¢ ciency tradeo¤s between di¤erent possible assumptions, at least in the production function context.},
   author = {Daniel A Ackerberg},
   title = {Timing Assumptions and E¢ ciency: Empirical Evidence in a Production Function Context},
   year = {2019},
}
@article{Ackerberg2021,
   abstract = {We revisit identification based on timing and information set assumptions in structural models, which have been used in the context of production functions, demand equations, and hedonic pricing models (e.g. Olley and Pakes (1996), Blundell and Bond (2000)). First, we demonstrate a general under-identification problem using these assumptions in a simple version of the Blundell-Bond dynamic panel model. In particular, the basic moment conditions can yield multiple discrete solutions: one at the persistence parameter in the main equation and another at the persistence parameter governing the regressor. Second, we propose possible solutions based on sign restrictions and augmented moments. We show the identification of our approach and propose a consistent estimation procedure. Our Monte Carlo simulations illustrate the under-identification issue and finite sample performance of our proposed estimator. Lastly, we show that the problem persists in many alternative models of the regressor but disappears in some models under stronger assumptions.},
   author = {Daniel Ackerberg and Garth Frazer and Yao Luo and Yingjun Su},
   keywords = {C18,D24,Identification,Market Persistence Factor,Monte Carlo Simulation JEL Classifications: C14,Production Function,Timing and Information Set Assumptions},
   title = {Under-Identification of Structural Models Based on Timing and Information Set Assumptions},
   url = {https://ssrn.com/abstract=3717757},
   year = {2021},
}
@article{Olley1996,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Technological change and deregulation have caused a major restructuring of the telecommunications equipment industry over the last two decades. Our empirical focus is on estimating the parameters of a production function for the equipment industry, and then using those estimates to analyze the evolution of plant-level productivity. The restructuring involved significant entry and exit and large changes in the sizes of incumbents. Firms' choices on whether to liquidate, and on input quantities should they continue, depended on their productivity. This generates a selection and a simultaneity problem when estimating production functions. Our theoretical focus is on providing an estimation algorithm which takes explicit account of these issues. We find that our algorithm produces markedly different and more plausible estimates of production function coefficients than do traditional estimation procedures. Using our estimates we find increases in the rate of aggregate productivity growth after deregulation. Since we have plant-level data we can introduce indices which delve deeper into how this productivity growth occurred. These indices indicate that productivity increases were primarily a result of a reallocation of capital towards more productive establishments.},
   author = {G Steven Olley and Ariel Pakes and G Steven},
   issue = {6},
   journal = {Econometrica},
   keywords = {Selection,productivity,simultaneity and production functions,telecommu-nications equipment and deregulation},
   pages = {1263-1297},
   title = {The Dynamics of Productivity in the Telecommunications Equipment Industry},
   volume = {64},
   year = {1996},
}
@article{Schennach2014,
   abstract = {This paper introduces a general method to convert a model defined by moment conditions that involve both observed and unobserved variables into equivalent mo- ment conditions that involve only observable variables. This task can be accomplished without introducing infinite-dimensional nuisance parameters using a least favorable entropy-maximizing distribution.We demonstrate, through examples and simulations, that this approach covers a wide class of latent variables models, including some game- theoretic models and models with limited dependent variables, interval-valued data, errors-in-variables, or combinations thereof. Both point- and set-identified models are transparently covered. In the latter case, the method also complements the recent lit- erature on generic set-inference methods by providing the moment conditions needed to construct a generalized method of moments-type objective function for a wide class of models. Extensions of the method that cover conditional moments, independence restrictions, and some state-space models are also given.},
   author = {Susane M. Schennach},
   doi = {10.3982/ecta9748},
   issn = {0012-9682},
   issue = {1},
   journal = {Econometrica},
   pages = {345-385},
   publisher = {The Econometric Society},
   title = {Entropic Latent Variable Integration via Simulation},
   volume = {82},
   year = {2014},
}

@article{Eilers1996,
   abstract = {B-splines are attractive for nonparametric modelling, but choosing the optimal number and positions of knots is a complex task. Equidistant knots can be used, but their small and discrete number allows only limited control over smoothness and fit. We propose to use a relatively large number of knots and a difference penalty on coefficients of adjacent B-splines. We show connections to the familiar spline penalty on the integral of the squared second derivative. A short overview of B-splines, of their construction and of penalized likelihood is presented. We discuss properties of penalized B-splines and propose various criteria for the choice of an optimal penalty parameter. Nonparametric logistic regression, density estimation and scatterplot smoothing are used as examples. Some details of the computations are presented.},
   author = {Paul H.C. Eilers and Brian D. Marx},
   doi = {https://doi.org/10.1214/ss/1038425655},
   issn = {0883-4237},
   issue = {2},
   journal = {Statistical Science},
   keywords = {Density estimation,Splines,generalized linear models,nonparametric models,smoothing,splines},
   month = {5},
   pages = {89-121},
   publisher = {Institute of Mathematical Statistics},
   title = {Flexible smoothing with B-splines and penalties},
   volume = {11},
   url = {https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.short},
   year = {1996}
}